Speaker 1  00:07
that's working. All right, here's another angle of this whole thing. multi threading mutexes shared data locks, race conditions, etc. So when you just a few observations that when you've got a lot of people working in the same type of environment and working on the same types of things that they need to coordinate things so that they don't, don't conflict on shared resources. And this can create a constraint because not only does the shared resource create the problem, create a bottleneck, but the actual act of checking for the shared resource makes it a bottleneck. Let me give you a practical example. Let's say that you've got one truck for two guys. It's not just the fact that only one guy can be on the road at a time. It's the fact that each guy has to check to see whether the other guy is using the truck before he can actually use the truck. Now in a digital world, if you're using something like Excel spreadsheets, for example, you have the same type of resource contention is what it's called. You have one person who wants to use the Excel spreadsheet and the other person doesn't now you can just try to open the Excel spreadsheet and that might be good enough but even the, you know, even the fact that you might not be able to open the Excel document creates introduces delay because you know, you're already Planning for Contingencies of what if I'm actually not able to or if it doesn't, completely throws those off sideways, what you're trying to do. mutexes are basically a way just say, okay, here are the parameters around you know, here, here are the parameters. You know, you can simplify this by creating parameters around how often or when you actually do try to access the shared resource. Now, sometimes you can engineer your shared resource in such a way that there's only one person responsible for actually managing and updating the shared resource. And because they have limited bandwidth. There's the it's a simple matter of sequencing in order so for, for example, in the vehicle example that I gave, the equivalent would be to have is to have a dispatcher where they make multiple requests. And whatever whatever form or mechanism that they want to prioritize it by whether it's by first come first serve, or whether it's by other types of prioritization. They can manage access to the shared resource and then ultimately, you know, there's a final final piece of this, which is there's a waiting that necessarily comes when you have a shared resource and so while you might engage with other with other types of sorry, what other types of activities you still The challenge is that, you know, when you when you've got multiple threads, you can very easily just tell them to wait until the shared resources available, but that's from a human perspective. That's horribly inefficient. And so, ultimately what you have to do is do this from a systems perspective. So you've got your notification or something like that that occurs when something becomes available.

Speaker 1  04:23
Call Waiting hold. These are types of examples of managing asynchronous communication because you've got a shared resource, ie either a person or a phone line. And you can either wait until the resources available or get a call back. I mean, this is I guess along with that, there's synchronous waiting and asynchronous waiting, has in literally, this is called a callback. And when a great example, again is customer service, when are you going to wait, wait, well, actually, there's three types. Three ways to do this. One is I'm going to there's synchronous waiting. There's event driven notifications, and then there's polling polling just means I'm gonna check check, check, check, check until it's done. Now. The problem with that? Well, and along with that, there's also timeout typically with the with any type of waiting, perhaps not with event. Driven but so you have

Speaker 1  06:03
to let's say that you've got a project that is an email that you send, and you don't get a response. Well, clearly, it's not a logical approach to simply wait until you get a response. So that means that you are either waiting for an event driven or pulling, pulling type of response. With this, there's also the concept of timeout. So let's say that you pull, which means that you periodically check back in and say, Hey, did I get an email? And if not, you might actually ping them and say, Hey, can I have an answer? To my email and you continue this event driven? You simply wait for the response to come back. And in the example of Google, it comes back to you after two days or three days, and you haven't gotten a response. So this guarantees some kind of timeout on the responses to your email messages. The same thing applies to so this is one of the ways that you prevent system failure, or that you protect against it. because the question is, how do you protect against things when they go completely dark? Well, you need contingencies for that. So what happens when.

Speaker 1  07:31
What happens when somebody is on hold for 10 minutes, and I didn't know they didn't just. stranded you ring them back into the main queue and you prioritize them to the tops of Cubans and it rings again

Unknown Speaker  07:45
you talk to somebody who's more likely to be available and says hey answer this call


Transcribed by https://otter.ai